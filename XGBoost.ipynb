{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx6cAQ3byYJ2nlSw5v/uAh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. XGBoost parameters\n",
        "\n",
        "XGBoost has 3 types of parameters: **General paramaters**, **booster paramaters** and **task paramaters**.\n",
        "\n",
        "### 1.1 Global configuration:\n",
        "- `verbosity`\n",
        "\n",
        "- `use_rmm` (True or False): Whether to use RAPIDS Memory Manager (RMM) to allocate cache GPU memory..\n",
        "\n",
        "### 1.2 General parameters:\n",
        "- `booster`: can be `gbtree`, `dart` or `gblinear`.\n",
        "\n",
        "- `device` : default=`cpu` else `cuda` or `gpu`.\n",
        "\n",
        "### 1.3 Parameters for tree booster:**\n",
        "- `learning_rate` : default=0.3 and range: [0,1]\n",
        "\n",
        "- `gamma` : [default=0] Gamma is a threshold that controls whether a tree is allowed to split a leaf node. When the model considers a split, it calculates how much the split reduces the loss (error). If the loss reduction is smaller than gamma, the split is NOT allowed.\n",
        "\n",
        "- `max_depth`: default=6\n",
        "\n",
        "| Dataset size (rows) | Overfitting risk | Recommended max_depth | Notes |\n",
        "|--------------------|------------------|------------------------|-------|\n",
        "| < 1,000            | Very high        | 2 – 3                  | Only simple interactions; rely on boosting |\n",
        "| 1k – 10k           | High             | 3 – 4                  | Strong regularization needed |\n",
        "| 10k – 50k          | Medium-high      | 4 – 5                  | Common tabular ML range |\n",
        "| 50k – 100k         | Medium           | 5 – 6                  | Default sweet spot |\n",
        "| 100k – 300k        | Medium-low       | 6 – 7                  | Slightly deeper trees allowed |\n",
        "| 300k – 1M          | Low              | 6 – 8                  | Increase depth carefully |\n",
        "| > 1M               | Very low         | 7 – 10                 | Only if features justify it |\n",
        "\n",
        "- `min_child_weight` (default=1) : min_child_weight controls how small a leaf node is allowed to be by requiring a minimum amount of data (or evidence) in each child after a split. A small value allows many tiny, specific splits (higher overfitting risk), while a larger value forces splits to be supported by more data, making the model more conservative.\n",
        "\n",
        "- `max_delta_step` (default=0) : Limits how big a correction a tree is allowed to make in one step. Think: “Don’t change your answer too much at once.” It prevents one tree from making a huge confidence jump based on very little data. Typical values: 1..10.\n",
        "\n",
        "**Instances sampling:**\n",
        "\n",
        "- `subsample` (default=1) : Percentage of data used during training. Typical values : 0.4 to 0.8.\n",
        "\n",
        "- `sampling_method` (default=uniform): The method used to sample data.\n",
        "\n",
        "    1. uniform\n",
        "    2. gradient_based :  Note that this sampling method is only supported when tree_method is set to hist and the device is cuda; other tree methods only support uniform sampling.\n",
        "\n",
        "**Features sampling:**\n",
        "\n",
        "- `colsample_bytree`, `colsample_bylevel`, `colsample_bynode` (default=1) : Sampling the features.\n",
        "\n",
        "**Regularization:**\n",
        "\n",
        "- `lambda` (default=1): Lambda is a weight shrinker. Denominator gets bigger so leaf weight gets smaller. L2 regularization adds a penalty to the loss function:\n",
        "\n",
        "$$\n",
        "\\text{Penalty} = \\lambda \\sum w^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "w = -\\frac{G}{H + \\lambda}\n",
        "$$\n",
        "| Dataset               | Suggested `lambda` |\n",
        "|-----------------------|--------------------|\n",
        "| Small dataset         | 5–20               |\n",
        "| Noisy data            | 10–50              |\n",
        "| Large dataset         | 1–5                |\n",
        "| High-dimensional data | 5–30               |\n",
        "\n",
        "- `alpha` : Increasing alpha makes the model more conservative by pushing leaf weights toward zero. Encourages exact zero weights, produces sparser models and acts like feature selection at the leaf level. Signs you need higher alpha: too many leaves with tiny effects, model reacts to noise, high variance across folds.\n",
        "\n",
        "| Situation                | Suggested `alpha` |\n",
        "|--------------------------|------------------|\n",
        "| Clean data               | 0–0.5            |\n",
        "| Some noise               | 0.5–2            |\n",
        "| High-dimensional data    | 1–10             |\n",
        "| Very noisy / sparse data | 5–50             |\n",
        "\n",
        "**Tree methods:**\n",
        "\n",
        "- `tree_method` (default= auto): XGBoost builds decision trees, and tree_method controls how it finds the best split at each node.\n",
        "\n",
        "  1. exact — Exact Greedy Algorithm: Tries every possible split point for every feature. Computes gain exactly.\n",
        "  2. approx — Approximate Greedy (Quantile Sketch): Uses quantile sketches to estimate good split points. Builds gradient histograms instead of scanning all values.\n",
        "  3. hist — Histogram-Based Algorithm (Recommended): Pre-buckets feature values into fixed bins (histograms). Finds splits using histogram statistics. Uses highly optimized C++ code.\n",
        "  4. auto — Automatic Choice (Same as hist): Currently behaves the same as hist. Keeps backward compatibility.\n",
        "\n",
        "\n",
        "\n",
        "- `max_leaves` [default=0] : Maximum number of nodes to be added. Not used by exact tree method.\n",
        "\n",
        "- `max_bin` [default=256]: Only used if tree_method is set to hist or approx. Maximum number of discrete bins to bucket continuous features. Increasing this number improves the optimality of splits at the cost of higher computation time.\n",
        "\n",
        "**Categorical features:**\n",
        "\n",
        "- `max_cat_to_onehot`: A threshold for deciding whether XGBoost should use one-hot encoding based split for categorical data. When number of categories is lesser than the threshold then one-hot encoding is chosen, otherwise the categories will be partitioned into children nodes.\n",
        "\n",
        "- `max_cat_threshold`: Maximum number of categories considered for each split. Used only by partition-based splits for preventing over-fitting.\n",
        "\n",
        "**Unbalanced data**:\n",
        "\n",
        "- scaloe_pos_weight: Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n",
        "\n",
        "### 1.4 Additional parameters for Dart Booster:\n",
        "\n",
        "How DART Works in XGBoost:\n",
        "\n",
        "1. **Train a few trees normally.**\n",
        "\n",
        "2. **When adding a new tree:**\n",
        "   - Randomly **drop some existing trees**.\n",
        "   - Compute the **gradient** using only the remaining trees.\n",
        "   - **Add the new tree** to the model.\n",
        "\n",
        "3. **Repeat** the process for all boosting rounds.\n",
        "\n",
        "This process helps make the model **more robust** and **reduces overfitting** by preventing the model from relying too heavily on any single tree.\n",
        "\n",
        "#### DART Booster Prediction in XGBoost:\n",
        "\n",
        "When using a **DART booster**, `predict()` **also applies dropouts by default**.  \n",
        "This is fine for training, but **on test/validation data**, it can produce **incorrect results** because not all trees are used.\n",
        "\n",
        "#### Problem:\n",
        "\n",
        "- DART randomly drops trees during **training** to reduce overfitting.\n",
        "- By default, `predict()` also applies dropout.\n",
        "- This makes predictions on new data **inaccurate**.\n",
        "\n",
        "#### Solution:\n",
        "\n",
        "Use the `iteration_range` parameter to **use all trained trees**:\n",
        "\n",
        "`preds = bst.predict(dtest, iteration_range=(0, num_round))`\n",
        "\n",
        "#### Parameters:\n",
        "\n",
        "- `sample_type` (default = `uniform`): Determines **how the trees to drop are selected** during dropout.\n",
        "\n",
        "| Option   | Meaning                                       | Example                                                                 |\n",
        "|----------|-----------------------------------------------|-------------------------------------------------------------------------|\n",
        "| uniform  | All existing trees have **equal chance** of being dropped | 10 trees → randomly pick 2 trees to drop, each tree equally likely     |\n",
        "| weighted | Trees with **higher weight** are more likely to be dropped | If some trees have higher weight (more impact), they are more likely to be dropped |\n",
        "\n",
        "- `normalize_type` (default = `tree`): Adjusts the **weight of new and dropped trees** so predictions stay consistent after dropout.\n",
        "\n",
        "| Option  | How weights are scaled                                                                 |\n",
        "|---------|----------------------------------------------------------------------------------------|\n",
        "| tree    | New tree weight = 1 / (k + learning_rate); dropped trees scaled by k / (k + learning_rate)  (k = number of dropped trees) |\n",
        "| forest  | New tree weight = 1 / (1 + learning_rate); dropped trees scaled by 1 / (1 + learning_rate) |\n",
        "\n",
        "\n",
        "- `rate_drop` [default=0.0]: Dropout rate (a fraction of previous trees to drop during the dropout). Range: [0.0, 1.0]\n",
        "\n",
        "- `skip_drop` [default=0.0]: Probability of skipping the dropout procedure during a boosting iteration.\n",
        "\n",
        "### 1.5 Learning task parameters:\n",
        "\n",
        "The learning task is the type of prediction you want (regression classification, ranking, survival analysis), and the objective is the loss function XGBoost will optimize during training. I’ll explain each with examples.\n",
        "\n",
        "- `objective` [default=reg:squarederror]:\n",
        "\n",
        "| Task               | Objective            | Output          | Example                        |\n",
        "|-------------------|-------------------|----------------|--------------------------------|\n",
        "| Regression         | reg:squarederror    | Continuous     | Predict house price            |\n",
        "| Regression         | reg:squaredlogerror | Continuous, >0 | Predict sales revenue          |\n",
        "| Regression         | reg:absoluteerror   | Continuous     | Predict delivery time          |\n",
        "| Regression         | reg:pseudohubererror| Continuous     | Predict stock prices with outliers |\n",
        "| Regression         | reg:quantileerror   | Continuous     | Predict 90th percentile delivery time |\n",
        "| Regression         | reg:gamma           | Continuous >0  | Insurance claim severity       |\n",
        "| Regression         | reg:tweedie         | Continuous ≥0  | Total insurance loss           |\n",
        "| Binary Classification | binary:logistic  | Prob 0–1       | Predict if customer buys a product |\n",
        "| Binary Classification | binary:logitraw   | Score (real)   | Predict likelihood of purchase |\n",
        "| Binary Classification | binary:hinge      | 0 or 1         | Spam detection                 |\n",
        "| Multi-class Classification | multi:softmax | Class label    | Predict digit (0–9)           |\n",
        "| Multi-class Classification | multi:softprob | Prob vector    | Image classification (cat/dog/rabbit) |\n",
        "| Ranking             | rank:pairwise      | Rank score     | Rank search results            |\n",
        "| Ranking             | rank:ndcg          | Rank score     | Search engine ranking (NDCG)  |\n",
        "| Ranking             | rank:map           | Rank score     | Recommendation ranking (MAP)  |\n",
        "| Survival Analysis   | survival:cox       | Hazard ratio   | Predict patient survival       |\n",
        "| Survival Analysis   | survival:aft       | Survival time  | Predict machine failure time   |\n",
        "| Count Data          | count:poisson      | Mean count     | Predict daily website clicks   |\n",
        "\n",
        "\n",
        "- `eval_metric` [default according to objective] :Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and logloss for classification )\n",
        "\n",
        "- `seed`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UcAhUzSgT0TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Interface"
      ],
      "metadata": {
        "id": "4PJ95VtmzK89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "QVz02Gd8zV4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.DataFrame(np.arange(12).reshape((4,3)), columns=['a', 'b', 'c'])\n",
        "label = pd.DataFrame(np.random.randint(2, size=4))\n",
        "test_data = train_data = pd.DataFrame(np.arange(12).reshape((4,3)), columns=['a', 'b', 'c'])"
      ],
      "metadata": {
        "id": "6Zh5PMev0FkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmGkFn-GTZZ2"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "dtrain = xgb.DMatrix(train_data, label=label)\n",
        "dtest = xgb.DMatrix(test_data, label=label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Setting parameters"
      ],
      "metadata": {
        "id": "xnBe_94lz-8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set params for the model:\n",
        "param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic', 'eval_metric': ['auc', 'ams@0']}"
      ],
      "metadata": {
        "id": "Lf12nJFcz-Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you train an XGBoost model, you can provide a list of datasets to evaluate performance during training. This is done via the evals parameter in xgb.train"
      ],
      "metadata": {
        "id": "yhzGkagV1IWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evallist = [(dtrain, 'train'), (dtest, 'eval')]"
      ],
      "metadata": {
        "id": "g_Lm_K_y1H0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training"
      ],
      "metadata": {
        "id": "HAZxASdn2c-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_round = 10\n",
        "bst = xgb.train(param, dtrain, num_round, evals=evallist, verbose_eval=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWvTJ8qb2cZJ",
        "outputId": "22175b70-db72-4f11-cee1-fb8f065128be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-auc:0.50000\ttrain-ams@0:0.00000\teval-auc:0.50000\teval-ams@0:0.00000\n",
            "[2]\ttrain-auc:0.50000\ttrain-ams@0:0.00000\teval-auc:0.50000\teval-ams@0:0.00000\n",
            "[4]\ttrain-auc:0.50000\ttrain-ams@0:0.00000\teval-auc:0.50000\teval-ams@0:0.00000\n",
            "[6]\ttrain-auc:0.50000\ttrain-ams@0:0.00000\teval-auc:0.50000\teval-ams@0:0.00000\n",
            "[8]\ttrain-auc:0.50000\ttrain-ams@0:0.00000\teval-auc:0.50000\teval-ams@0:0.00000\n",
            "[9]\ttrain-auc:0.50000\ttrain-ams@0:0.00000\teval-auc:0.50000\teval-ams@0:0.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Saving\n"
      ],
      "metadata": {
        "id": "GS4lhrTx3r6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bst.save_model('model.ubj')"
      ],
      "metadata": {
        "id": "rqDzsENIzo1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load Model"
      ],
      "metadata": {
        "id": "o_4zbOYF362I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bst = xgb.Booster()  # init model\n",
        "bst.load_model('model.ubj')  # load model data"
      ],
      "metadata": {
        "id": "hnlge00a36qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping\n",
        "\n",
        "If you have a validation set, you can use early stopping to find the optimal number of boosting rounds. Early stopping requires at least one set in evals. If there’s more than one, it will use the last.\n",
        "\n",
        "`train(..., evals=evals, early_stopping_rounds=10)`\n",
        "\n",
        "The model will train until the validation score stops improving. Validation error needs to decrease at least every early_stopping_rounds to continue training.\n",
        "\n",
        "Prediction for early stopping:\n",
        "\n",
        "`ypred = bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1))`"
      ],
      "metadata": {
        "id": "7Wd5YJJT4Kr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks\n",
        "\n",
        "This [link](https://xgboost.readthedocs.io/en/stable/python/callbacks.html) can help us get more information about callbacks.\n",
        "\n",
        "XGBoost provides an callback interface class: TrainingCallback, user defined callbacks should inherit this class and override corresponding methods. There’s a working example in [Demo for using and defining callback functions](https://xgboost.readthedocs.io/en/stable/python/examples/callbacks.html#sphx-glr-python-examples-callbacks-py)."
      ],
      "metadata": {
        "id": "bVfYE0dn66OO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using xgboost on GPU devices\n",
        "\n",
        "Shows how to train a model on the [forest cover type](https://archive.ics.uci.edu/ml/datasets/covertype) dataset using GPU\n",
        "acceleration. The forest cover type dataset has 581,012 rows and 54 features, making it\n",
        "time consuming to process. We compare the run-time and accuracy of the GPU and CPU\n",
        "histogram algorithms.\n",
        "\n",
        "In addition, The demo showcases using GPU with other GPU-related libraries including\n",
        "cupy and cuml. These libraries are not strictly required."
      ],
      "metadata": {
        "id": "Hl51oLcA8o8I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34p7Pk9_92Xl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}