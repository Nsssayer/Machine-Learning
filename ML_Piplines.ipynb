{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1fUfrGmKFAYZ7qMUdapkOvxKNx_75ApUA",
      "authorship_tag": "ABX9TyMQHyFFSbgt/NKi8s3453w+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5VgU9J3e2gJ",
        "outputId": "a79e4da8-ac26-4273-a3dd-4da959cbba6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:19: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:19: SyntaxWarning: invalid escape sequence '\\('\n",
            "/tmp/ipython-input-284490091.py:19: SyntaxWarning: invalid escape sequence '\\('\n",
            "  url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# from custom_transformer import StartingVerbExtractor\n",
        "\n",
        "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What is CountVectorizer?\n",
        "\n",
        "CountVectorizer converts a collection of text documents into a matrix of token counts:\n",
        "\n",
        "- tokenizes (splits text into words)\n",
        "\n",
        "- builds a vocabulary\n",
        "\n",
        "- counts how often each word appears\n",
        "\n",
        "- outputs a document-term matrix"
      ],
      "metadata": {
        "id": "qOfR9Yp_kiAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text documents\n",
        "documents = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is awesome\",\n",
        "    \"I love programming and learning\"\n",
        "]\n",
        "\n",
        "# 1. Create the vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# 2. Fit and transform\n",
        "X = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "iJQhGduek7rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Show vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gonPLqrBlCPK",
        "outputId": "2ab6d12d-2f9c-473b-96aa-3dd52439f102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'awesome' 'is' 'learning' 'love' 'machine' 'programming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Show document-term matrix\n",
        "print(\"Document-term matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA6Fa-IKlJvj",
        "outputId": "6b80ed9d-0a45-44ee-a2cb-a177fb555ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-term matrix:\n",
            " [[0 0 0 1 1 1 0]\n",
            " [0 1 1 1 0 1 0]\n",
            " [1 0 0 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What TfidfTransformer Does\n",
        "\n",
        "TfidfTransformer converts a raw count matrix (produced by CountVectorizer or any count-based representation) into TF-IDF scores.\n",
        "\n",
        "TF (Term Frequency): how often a word appears in a document\n",
        "\n",
        "IDF (Inverse Document Frequency): how “unique” a word is across documents\n",
        "\n",
        "TF-IDF = TF × IDF → higher for important/rare words, lower for very common words"
      ],
      "metadata": {
        "id": "GfL2dkg-m9vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog sat on the log\"\n",
        "]\n",
        "\n",
        "# Step 1: Convert documents to raw counts\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform(docs)\n",
        "\n",
        "print(\"Count matrix:\\n\", X_counts.toarray())\n",
        "print(\"Vocabulary:\", count_vect.vocabulary_)\n",
        "\n",
        "# Step 2: Transform count matrix into TF-IDF\n",
        "tfidf = TfidfTransformer()\n",
        "X_tfidf = tfidf.fit_transform(X_counts)\n",
        "\n",
        "print(\"\\nTF-IDF matrix:\\n\", X_tfidf.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOy6wScunEF_",
        "outputId": "5d41715d-112d-4255-b149-8383f71be588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count matrix:\n",
            " [[1 0 0 1 1 1 2]\n",
            " [0 1 1 0 1 1 2]]\n",
            "Vocabulary: {'the': 6, 'cat': 0, 'sat': 5, 'on': 4, 'mat': 3, 'dog': 1, 'log': 2}\n",
            "\n",
            "TF-IDF matrix:\n",
            " [[0.44554752 0.         0.         0.44554752 0.31701073 0.31701073\n",
            "  0.63402146]\n",
            " [0.         0.44554752 0.44554752 0.         0.31701073 0.31701073\n",
            "  0.63402146]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Machine Learning Workflow"
      ],
      "metadata": {
        "id": "2vWhdMWdgAsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
        "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
        "    X = df.text.values\n",
        "    y = df.category.values\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "K-DWakoqfPT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    detected_urls = re.findall(url_regex, text)\n",
        "    for url in detected_urls:\n",
        "        text = text.replace(url, \"urlplaceholder\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean_tokens = []\n",
        "    for tok in tokens:\n",
        "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
        "        clean_tokens.append(clean_tok)\n",
        "\n",
        "    return clean_tokens"
      ],
      "metadata": {
        "id": "243ou41ygDOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(y_test, y_pred):\n",
        "    labels = np.unique(y_pred)\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "    accuracy = (y_pred == y_test).mean()\n",
        "\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
        "    print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "VB3gbiKCgKEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    X, y = load_data()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    vect = CountVectorizer(tokenizer=tokenize)\n",
        "    tfidf = TfidfTransformer()\n",
        "    clf = RandomForestClassifier()\n",
        "\n",
        "    # train classifier\n",
        "    X_train_counts = vect.fit_transform(X_train)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # predict on test data\n",
        "    X_test_counts = vect.transform(X_test)\n",
        "    X_test_tfidf = tfidf.transform(X_test_counts)\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "    # display results\n",
        "    display_results(y_test, y_pred)"
      ],
      "metadata": {
        "id": "EgIi0BjSgWvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Build a pipeline"
      ],
      "metadata": {
        "id": "9GPc-X5KpJ77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  X, y = load_data()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "  # create a pipeline:\n",
        "  pipeline = Pipeline([\n",
        "      ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "      ('tfidf', TfidfTransformer()),\n",
        "      ('clf', RandomForestClassifier())\n",
        "  ])\n",
        "\n",
        "  # train a classifier:\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  # predict:\n",
        "  y_preds = pipeline.predict(X_test)\n",
        "\n",
        "  # display results\n",
        "  display_results(y_test, y_pred)"
      ],
      "metadata": {
        "id": "LlDZ3r4En8T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Creating Custom Transformer\n",
        "\n",
        "Remember, all estimators have a fit method, and since this is a transformer, it also has a transform method.\n",
        "\n",
        "- FIT METHOD: This takes in a 2d array X for the feature data and a 1d array y for the target labels. Inside the fit method, we simply return self. This allows us to chain methods together, since the result on calling fit on the transformer is still the transformer object. This method is required to be compatible with scikit-learn.\n",
        "\n",
        "- TRANSFORM METHOD: The transform function is where we include the code that well, transforms the data. In this case, we return the data in X multiplied by 10. This transform method also takes a 2d array X.\n"
      ],
      "metadata": {
        "id": "SuG787CjtwBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TenMultiplier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X * 10"
      ],
      "metadata": {
        "id": "DijZluOYs_fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multiplier = TenMultiplier()\n",
        "X = np.array([6, 3, 7, 4, 7])\n",
        "multiplier.transform(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga2GaJ5IuMD-",
        "outputId": "a0c0c08b-059f-49ea-bb2e-e7d4872ef060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([60, 30, 70, 40, 70])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def starting_verb(self, text):\n",
        "        sentence_list = nltk.sent_tokenize(text)\n",
        "        for sentence in sentence_list:\n",
        "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
        "            first_word, first_tag = pos_tags[0]\n",
        "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
        "        return pd.DataFrame(X_tagged)"
      ],
      "metadata": {
        "id": "NNtdxypwuQ1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Feature Union"
      ],
      "metadata": {
        "id": "7uMTA1pN7IWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pipeline():\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "\n",
        "            ('text_pipeline', Pipeline([\n",
        "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "                ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "\n",
        "            ('starting_verb', StartingVerbExtractor())\n",
        "        ])),\n",
        "\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    return pipeline\n"
      ],
      "metadata": {
        "id": "PJn_Bt_e61ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    X, y = load_data()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    model = model_pipeline()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    display_results(y_test, y_pred)"
      ],
      "metadata": {
        "id": "VoTkOPyh7QF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Grid Search Pipeline"
      ],
      "metadata": {
        "id": "cRklsk1C7c2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "\n",
        "            ('text_pipeline', Pipeline([\n",
        "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "                ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "\n",
        "            ('starting_verb', StartingVerbExtractor())\n",
        "        ])),\n",
        "\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    parameters = {\n",
        "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
        "        'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
        "        'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
        "        'features__text_pipeline__tfidf__use_idf': (True, False),\n",
        "        'clf__n_estimators': [50, 100, 200],\n",
        "        'clf__min_samples_split': [2, 3, 4],\n",
        "        'features__transformer_weights': (\n",
        "            {'text_pipeline': 1, 'starting_verb': 0.5},\n",
        "            {'text_pipeline': 0.5, 'starting_verb': 1},\n",
        "            {'text_pipeline': 0.8, 'starting_verb': 1},\n",
        "        )\n",
        "    }\n",
        "\n",
        "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
        "\n",
        "    return cv"
      ],
      "metadata": {
        "id": "RtcGG5ml7SdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    X, y = load_data()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    model = build_model()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    display_results(model, y_test, y_pred)"
      ],
      "metadata": {
        "id": "R2erEbK47wOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05cBmOR67y7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}